task: "cola"
metric: "accuracy"
precision: 32
tokenizer: "bert-base-uncased"
batch_size: 16
max_length: 512
num_epoch: 10

model:
  attention: "linear"
  num_classes: 2
  d_model: 256
  num_heads: 4
  dim_feedforward: 1024
  num_layers: 2
  dropout: 0.5

optimizer:
  lr: 1e-4
  T_max: 0

trainer:
  accelerator: "gpu"
  accumulate: 4
  log_steps: 50

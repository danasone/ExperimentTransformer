task: "cola"
metric: "accuracy"
precision: 32
tokenizer: "bert-base-uncased"
batch_size: 16
max_length: 512
num_epoch: 3

model:
  attention: "linear"
  num_classes: 2
  d_model: 768
  num_heads: 12
  dim_feedforward: 3072
  num_layers: 12
  dropout: 0.1

optimizer:
  lr: 1e-4
  T_max: 0

trainer:
  accelerator: "gpu"
  accumulate: 4
  log_steps: 50
